# -*- coding: utf-8 -*-
"""Analysis tweet by twint.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1sy4ASTs2yhBnm3aDTs4b8_fg2sRPWMvD

**GETTING THE USER DETAILS AND SAVING THE DATA TO A CSV FILE**
"""

!pip install twint

import twint 
from datetime import date

today1 = str(date.today())
print("Today's date:", today1)

first_day=str(date.today().replace(day=1))
print(first_day)
name_of_user=input("HI THERE! PLEASE ENTER YOUR TWITTER USERNAME\n")
c=twint.Config()
c.Username = "{}".format(name_of_user)
c.Store_csv=True
c.Output="data_user.csv"
c.Since=first_day
c.Lang='en'
c.Until=today1
twint.run.Search(c)

import pandas as pd 
import warnings
import re 
import nltk 
import string

warnings.filterwarnings('ignore')

data=pd.read_csv("data_user.csv")
data.shape

# from datetime import date
# today_main=date.today()
# data['date'] = pd.to_datetime(data['date'])
# data=data[(data.date.dt.month==today_main.month)]

data.head()

data.shape

from nltk.corpus import sentiwordnet as swn
from nltk import pos_tag, map_tag
from nltk.stem import WordNetLemmatizer
from nltk.stem import PorterStemmer
import nltk
nltk.download('punkt')
nltk.download('averaged_perceptron_tagger')
nltk.download('universal_tagset')
nltk.download('sentiwordnet')
nltk.download('wordnet')

"""**FINDING THE SENTIMENT OF EACH TWEET**"""

lem = WordNetLemmatizer()
pstem = PorterStemmer()

def pos_senti(data):#takes
    li_swn=[]
    li_swn_pos=[]
    li_swn_neg=[]
    missing_words=[]
    for i in range(len(data.index)):
        text = data.loc[i]['tweet']
        tokens = nltk.word_tokenize(text)
        tagged_sent = pos_tag(tokens)
        store_it = [(word, map_tag('en-ptb', 'universal', tag)) for word, tag in tagged_sent]
        #print("Tagged Parts of Speech:",store_it)

        pos_total=0
        neg_total=0
        for word,tag in store_it:
            if(tag=='NOUN'):
                tag='n'
            elif(tag=='VERB'):
                tag='v'
            elif(tag=='ADJ'):
                tag='a'
            elif(tag=='ADV'):
                tag = 'r'
            else:
                tag='nothing'

            if(tag!='nothing'):
                concat = word+'.'+tag+'.01'
                try:
                    this_word_pos=swn.senti_synset(concat).pos_score()
                    this_word_neg=swn.senti_synset(concat).neg_score()
                    #print(word,tag,':',this_word_pos,this_word_neg)
                except Exception as e:
                    wor = lem.lemmatize(word)
                    concat = wor+'.'+tag+'.01'
                    # Checking if there's a possiblity of lemmatized word be accepted into SWN corpus
                    try:
                        this_word_pos=swn.senti_synset(concat).pos_score()
                        this_word_neg=swn.senti_synset(concat).neg_score()
                    except Exception as e:
                        wor = pstem.stem(word)
                        concat = wor+'.'+tag+'.01'
                        # Checking if there's a possiblity of lemmatized word be accepted
                        try:
                            this_word_pos=swn.senti_synset(concat).pos_score()
                            this_word_neg=swn.senti_synset(concat).neg_score()
                        except:
                            missing_words.append(word)
                            continue
                pos_total+=this_word_pos
                neg_total+=this_word_neg
        li_swn_pos.append(pos_total)
        li_swn_neg.append(neg_total)

        if(pos_total!=0 or neg_total!=0):
            if(pos_total>neg_total):
                li_swn.append(1)
            else:
                li_swn.append(-1)
        else:
            li_swn.append(0)
            
    data['pos_score']=li_swn_pos
    data['neg_score']=li_swn_neg
    data['sentiment_score']=li_swn
    return data


data=pos_senti(data)

data1=data.copy()

"""**CHECK FOR RETWEETS AND IGNORES THE ONES WHICH ARE RETWEETS**"""

data.tweet.str.contains('@RT').unique()
data=data[data.retweet==False]

data.tweet=data.tweet.str.lower()
data['tweet']

"""**DROPPING THE DUPLICATE TWEETS**"""

data=data.drop_duplicates(subset='tweet',keep='first')

data.shape

"""**analyze tweets by two steps. First, preprocessing tweets by Ignoring a duplicate words or tweets, medical or educational words, numbers, URLs, mentions, images, punctuation and spaces and removes tweets that contain advertisements, and retweet tweets and filters the dataset by dictionary (Sentiwordnet) and calculating the sentiment of each tweet.**
Second, classifier tweet by niave bayes algorithm and show results monthly, weekly and daily percent**

**REMOVING WORDS RELATED TO EDUCATION** 

I have taken the words from https://www.mondovo.com/keywords/education-keywords
"""

education_dataframe=pd.read_csv("education_keywords.csv")
education=list(education_dataframe['keywords'])

def education_removal(text): 
    inter = list(text.split(" "))
    for i in inter:
        if i in education:
            inter.remove(i)
    text=' '.join(inter)
    return text

data['tweet'] = data['tweet'].apply(lambda x: education_removal(x))

"""**REMOVING WORDS RELATED TO MEDICAL** 

I have taken the words from https://figshare.com/articles/List_of_Health_Keywords/1084358/1
"""

medical_dataframe=pd.read_csv("medical_keywords.csv")
medical_dataframe

def remove_punct(text):
    text  = "".join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', '', text)
    return text

medical_dataframe['keywords'] = medical_dataframe['keywords'].apply(lambda x: remove_punct(x))

medical=list(medical_dataframe['keywords'])
medical.remove("depression")

def medical_removal(text): 
    inter = list(text.split(" "))
    for i in inter:
        if i in medical:
            inter.remove(i)
    text=' '.join(inter)
    return text
    
    

data['tweet'] = data['tweet'].apply(lambda x: medical_removal(x))



"""**REMOVING URLS**"""

def remove_url(text):

    text = re.sub(r"http\S+", "", text)
    text= re.sub(r"https\S+", "", text)
    return text


data['tweet'] = data['tweet'].apply(lambda x: remove_url(x))

data['tweet'][3]

"""**REMOVING PHOTOS**"""

def remove_photos(text):

    text = re.sub(r"pic.twitter\S+", "", text)
    return text


data['tweet'] = data['tweet'].apply(lambda x: remove_photos(x))

data['tweet'][10]

"""**REMOVING MENTIONS**"""

def remove_mentions(text):
    inter = list(text.split(" "))
    for i in inter: 
        if i.startswith("@"):
            inter.remove(i)
    text=' '.join(inter)
    return text

data['tweet'] = data['tweet'].apply(lambda x: remove_mentions(x))

"""**REMOVING ADVERTISEMENTS**"""

data=data.drop(data[data.tweet.str.contains("#ad")].index)
data=data.drop(data[data.tweet.str.contains("#sponsored")].index)

data.shape

"""**REMOVING HASHTAGS WORDS**"""

def remove_hashtag(text):
    inter = list(text.split(" "))
    for i in inter: 
        if i.startswith("#"):
            inter.remove(i)
    text=' '.join(inter)
    return text

data['tweet'] = data['tweet'].apply(lambda x: remove_hashtag(x))

"""**REMOVING NUMBERS, PUNCTIONS, TOKENISATION, STOPWORDS, SYMBOLS**

Remove punctuations

Tokenization - Converting a sentence into list of words

Remove stopwords
"""

import string
import re
import nltk

"""**REMOVING PUNCTUATIONS & SYMBOLS & NUMBERS**"""

import nltk
nltk.download('stopwords')

string.punctuation

"""puncuation and numbers"""

def remove_punct(text):
    text  = "".join([char for char in text if char not in string.punctuation])
    text = re.sub('[0-9]+', '', text)
    return text

data['tweet'] = data['tweet'].apply(lambda x: remove_punct(x))

"""**REMOVING TOKENIZATION**"""

def tokenization(text):
    text = re.split('\W+', text)
    return text

data['tweet'] = data['tweet'].apply(lambda x: tokenization(x.lower()))
data.head()

data['tweet']

"""**REMOVING STOP WORDS**"""

stopword = nltk.corpus.stopwords.words('english')

def remove_stopwords(text):
    text = [word for word in text if word not in stopword]
    return text
    
data['tweet'] = data['tweet'].apply(lambda x: remove_stopwords(x))
data['tweet']

"""**DOING STEMMING**"""

ps = nltk.PorterStemmer()

def stemming(text):
    text = [ps.stem(word) for word in text]
    return text

data['tweet'] = data['tweet'].apply(lambda x: stemming(x))
data['tweet']

"""**DOING LEMMATIZATION**"""

import nltk
nltk.download('wordnet')

wn = nltk.WordNetLemmatizer()

def lemmatizer(text):
    text = [wn.lemmatize(word) for word in text]
    return text

data['tweet'] = data['tweet'].apply(lambda x: lemmatizer(x))
data['tweet']

data[['tweet']]

"""**JOINING THE WORDS TO CREATE A SENTENCE BACK AGAIN**"""

data['tweet']=data['tweet'].str.join(" ")
data['tweet']

"""**REMOVING DUPLICATE WORDS**"""

from collections import OrderedDict

data['tweet'] = (data['tweet'].str.split().apply(lambda x: OrderedDict.fromkeys(x).keys()).str.join(' '))
data

"""**MAKING NEW DATAFRAME FOR OPERATION**"""

new_data=data[['tweet','likes_count','replies_count','pos_score','neg_score','sentiment_score']]

new_data

"""**DOING PROCESS OF VECTORIZATION AND APPLYING THE ALGORITHM**"""

from nltk.corpus import stopwords
from sklearn.feature_extraction.text import TfidfVectorizer

vectorizer = TfidfVectorizer (max_features=1000, min_df=7, max_df=0.8, stop_words=stopwords.words('english'))
new_data['tweet'] = vectorizer.fit_transform(new_data['tweet']).toarray()

new_data['tweet'].unique()

labels=new_data['sentiment_score']
value_to_pass=new_data.drop(columns=['sentiment_score'])

from sklearn.model_selection import train_test_split

X_train, X_test, y_train, y_test = train_test_split(value_to_pass, labels, test_size=0.4, random_state=0)

from sklearn.naive_bayes import GaussianNB

#Create a Gaussian Classifier
model = GaussianNB()

# Train the model using the training sets
model.fit(value_to_pass,labels)

predicted=list(model.predict(X_test))

"""**DETERMINING IF THE PERSON IS DEPRESSED OR NOT**"""

if predicted.count(1)>predicted.count(-1):
    print('The person is NOT DEPRESSED')
elif predicted.count(1)==predicted.count(-1):
    print("The person has a NEUTRAL sentiment")
elif predicted.count(1)<predicted.count(-1):
    print('The person is DEPRESSED')

"""**SHOWCASING THE MONTHLY, WEEKLY, DAILY TWEETS**"""

data

"""**MAKING A NEW DATAFRAME WITH DATE AND TIME VALUES**"""

calender=data[['tweet','likes_count','replies_count','pos_score','neg_score','sentiment_score','date','time']]

# calender['date']=calender['date'].astype("datetime64[ns]")
calender.shape

calender.date.dtype

"""**FINDING OUT TODAY'S DAY**"""

from datetime import date

today = date.today()
print("Today's date:", today)

save=list(calender['date'])

"""**GETTING THE NUMBER OF DAYS BEFORE A TWEET WAS TWEETED**

**OBTAINING DAILY TWEETS**
"""

data1['date'] = pd.to_datetime(data1['date'])
daily=data1[(data1.date.dt.month==today.month) & (data1.date.dt.day==today.day) & (data1.sentiment_score==-1)]
daily_tweet=list(daily['tweet'])

daily_json=json.dumps(daily_tweet)
print(daily_json)

with open("dail.json", "w") as write_file:
    json.dump(daily_json, write_file)

"""**OBTAIING WEEKLY TWEETS**"""

import datetime
import json
weekno=datetime.datetime.utcnow().isocalendar()[1]
# print(weekno)
data1['date'] = pd.to_datetime(data1['date'])
weekly=data1[(data1.date.dt.week==weekno) & (data1.sentiment_score==-1)]
weekly_tweet=list(weekly['tweet'])

weekly_json=json.dumps(weekly_tweet)
print(weekly_json)

with open("week.json", "w") as write_file:
    json.dump(weekly_json, write_file)

"""**OBTAINING MONTHLY TWEETS**"""

data1['date'] = pd.to_datetime(data1['date'])
monthly=data1[(data1.date.dt.month==today.month) & (data1.sentiment_score==-1)]


monthly_tweet=list(monthly['tweet'])

monthly_json=json.dumps(monthly_tweet)
print(monthly_json)


with open("month.json", "w") as write_file:
    json.dump(monthly_json, write_file)

! pip install pyphi

f=open("{}.json".format(name_of_user),"w")

main1={"daily":daily_json,"weekly":weekly_json ,"monthly":monthly_json}

from pyphi import jsonify

jsonify.dump(main1,f)
f.close()

